# Ollama UI - رابط کاربری فارسی برای Ollama

یک رابط کاربری ساده و مینیمال برای ارتباط با مدل‌های Ollama به زبان فارسی.

## ویژگی‌ها

- ✅ رابط کاربری مینیمال و زیبا مشابه Open WebUI
- ✅ پشتیبانی کامل از زبان فارسی (RTL)
- ✅ استریمینگ پاسخ‌ها به صورت Real-time
- ✅ نمایش فرآیند Thinking
- ✅ قابلیت توقف پاسخ
- ✅ پشتیبانی از Markdown و فرمول‌های ریاضی (LaTeX)
- ✅ نمایش باکس‌های کد با Syntax Highlighting
- ✅ انتخاب مدل از فایل JSON
- ✅ تنظیم سیستم پرامپت
- ✅ حفظ تاریخچه چت در هر سشن

## پیش‌نیازها

1. **Python 3.8+**
2. **Ollama** نصب شده و در حال اجرا

## نصب و راه‌اندازی

### مرحله 1: نصب Ollama

اگر Ollama را نصب نکرده‌اید:

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
از [ollama.com](https://ollama.com) دانلود و نصب کنید.

### مرحله 2: راه‌اندازی Ollama

#### شروع سرویس Ollama

در یک ترمینال، دستور زیر را اجرا کنید:

```bash
ollama serve
```

این دستور سرویس Ollama را روی پورت پیش‌فرض `11434` راه‌اندازی می‌کند.

#### دانلود مدل Qwen3

در یک ترمینال جدید (یا همان ترمینال اگر سرویس را در background اجرا کردید):

```bash
ollama pull qwen3
```

این دستور مدل Qwen3 را دانلود می‌کند. بسته به سرعت اینترنت شما، این فرآیند ممکن است چند دقیقه طول بکشد.

#### بررسی مدل‌های نصب شده

برای دیدن لیست مدل‌های نصب شده:

```bash
ollama list
```

### مرحله 3: راه‌اندازی Ollama UI

#### روش 1: استفاده از اسکریپت (پیشنهادی)

```bash
./run.sh
```

#### روش 2: دستی

```bash
# فعال‌سازی محیط مجازی (اگر از venv استفاده می‌کنید)
source venv/bin/activate  # macOS/Linux
# یا
venv\Scripts\activate  # Windows

# نصب وابستگی‌ها (اگر قبلاً نصب نشده)
pip install -r requirements.txt

# راه‌اندازی سرور
python main.py
```

یا با uvicorn:

```bash
uvicorn main:app --host 127.0.0.1 --port 8000 --reload
```

### مرحله 4: دسترسی به رابط کاربری

مرورگر را باز کنید و به آدرس زیر بروید:

```
http://localhost:8000
```

## پیکربندی

### تنظیم آدرس Ollama

به صورت پیش‌فرض، برنامه به `http://localhost:11434` متصل می‌شود. اگر Ollama شما روی آدرس دیگری اجرا می‌شود، می‌توانید با متغیر محیطی تنظیم کنید:

**macOS/Linux:**
```bash
export OLLAMA_HOST=http://localhost:11434
python main.py
```

**Windows:**
```cmd
set OLLAMA_HOST=http://localhost:11434
python main.py
```

### افزودن مدل‌های جدید

1. مدل را در Ollama دانلود کنید:
   ```bash
   ollama pull llama3.2
   ollama pull mistral
   ```

2. فایل `models.json` را ویرایش کنید و مدل جدید را اضافه کنید:

```json
{
  "models": [
    {
      "id": "qwen3:4B",
      "name": "Qwen 3 (4B)",
      "description": "مدل Qwen 3 برای چت و تولید متن"
    },
    {
      "id": "llama3.2",
      "name": "Llama 3.2",
      "description": "مدل Llama 3.2"
    }
  ],
  "default": "qwen3:4B"
}
```

## استفاده

1. **انتخاب مدل**: از منوی بالای صفحه یک مدل انتخاب کنید
2. **ارسال پیام**: پیام خود را در باکس ورودی تایپ کنید و Enter بزنید یا روی دکمه ارسال کلیک کنید
3. **پرامپت سیستم**: برای تنظیم پرامپت سیستم، روی آیکون تنظیمات در باکس ورودی کلیک کنید
4. **توقف پاسخ**: در حین دریافت پاسخ، می‌توانید با کلیک روی دکمه توقف، پاسخ را متوقف کنید
5. **چت جدید**: برای شروع یک چت جدید، روی آیکون چت جدید در بالای سمت راست کلیک کنید

## ساختار پروژه

```
Ollama-UI/
├── main.py              # سرور FastAPI
├── models.json          # فایل مدل‌های موجود
├── requirements.txt     # وابستگی‌های Python
├── static/
│   ├── index.html      # صفحه اصلی
│   ├── styles.css      # استایل‌ها
│   └── app.js          # منطق فرانت‌اند
└── README.md           # این فایل
```

## عیب‌یابی

### مشکل: "خطا در ارتباط با Ollama"

**راه‌حل:**
1. مطمئن شوید Ollama در حال اجرا است:
   ```bash
   # بررسی سرویس
   curl http://localhost:11434/api/tags
   ```
   
   اگر خطا داد، Ollama را راه‌اندازی کنید:
   ```bash
   ollama serve
   ```

2. بررسی کنید که مدل نصب شده باشد:
   ```bash
   ollama list
   ```
   
   اگر مدل وجود ندارد:
   ```bash
   ollama pull qwen3
   ```

3. اگر Ollama روی پورت دیگری اجرا می‌شود، متغیر محیطی `OLLAMA_HOST` را تنظیم کنید

### مشکل: مدل‌ها در UI نمایش داده نمی‌شوند

**راه‌حل:**
1. فایل `models.json` را بررسی کنید
2. مطمئن شوید مدل‌های تعریف شده در Ollama نصب شده‌اند
3. Console مرورگر را بررسی کنید (F12) برای خطاهای JavaScript

### مشکل: پاسخ‌ها استریم نمی‌شوند

**راه‌حل:**
1. Console مرورگر را باز کنید (F12) و بررسی کنید که WebSocket متصل شده باشد
2. مطمئن شوید که Ollama از استریمینگ پشتیبانی می‌کند (نسخه جدید)
3. بررسی کنید که فایروال یا proxy مانع ارتباط WebSocket نشده باشد

### مشکل: پرامپت سیستم کار نمی‌کند

**راه‌حل:**
1. مطمئن شوید که پرامپت را در باکس مربوطه وارد کرده‌اید
2. بعد از وارد کردن پرامپت، باکس را ببندید
3. پرامپت برای تمام پیام‌های بعدی اعمال می‌شود

## نکات مهم

1. **سرویس Ollama باید همیشه در حال اجرا باشد** - اگر Ollama را ببندید، UI نمی‌تواند با مدل‌ها ارتباط برقرار کند

2. **مدل‌ها فضای زیادی اشغال می‌کنند** - مدل Qwen3 حدود 2-4 GB فضا نیاز دارد

3. **اولین استفاده ممکن است کند باشد** - مدل‌ها برای اولین بار باید در حافظه بارگذاری شوند

4. **استفاده از GPU** - اگر GPU دارید، Ollama به صورت خودکار از آن استفاده می‌کند

## تست اتصال

برای تست اینکه همه چیز به درستی کار می‌کند:

```bash
# تست API Ollama
curl http://localhost:11434/api/tags

# تست مدل
ollama run qwen3 "سلام"
```

اگر این دستورات کار کردند، همه چیز آماده است!

## پشتیبانی

اگر مشکلی پیش آمد:
1. Console مرورگر را بررسی کنید (F12)
2. لاگ‌های سرور را بررسی کنید
3. مطمئن شوید Ollama در حال اجرا است

## مجوز

این پروژه برای استفاده شخصی و آموزشی است.

## مشارکت

برای گزارش باگ یا پیشنهاد ویژگی جدید، لطفاً Issue ایجاد کنید.
