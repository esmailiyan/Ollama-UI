# Ollama UI - رابط کاربری فارسی برای Ollama

یک رابط کاربری ساده و مینیمال برای ارتباط با مدل‌های Ollama به زبان فارسی.

## ویژگی‌ها

- ✅ رابط کاربری مینیمال و زیبا مشابه Open WebUI
- ✅ پشتیبانی کامل از زبان فارسی (RTL)
- ✅ استریمینگ پاسخ‌ها به صورت Real-time
- ✅ نمایش فرآیند Thinking
- ✅ قابلیت توقف پاسخ
- ✅ پشتیبانی از Markdown و فرمول‌های ریاضی (LaTeX)
- ✅ نمایش باکس‌های کد با Syntax Highlighting
- ✅ انتخاب مدل از فایل JSON
- ✅ تنظیم سیستم پرامپت
- ✅ حفظ تاریخچه چت در هر سشن

## پیش‌نیازها

1. **Python 3.8+**
2. **Ollama** نصب شده و در حال اجرا

## نصب و راه‌اندازی

### 1. نصب Ollama

اگر Ollama را نصب نکرده‌اید:

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
دانلود از [ollama.com](https://ollama.com)

### 2. راه‌اندازی Ollama

```bash
# شروع سرویس Ollama
ollama serve
```

در ترمینال دیگری:

```bash
# دانلود مدل Qwen3 (یا هر مدل دیگری)
ollama pull qwen3
```

### 3. نصب وابستگی‌های پروژه

```bash
# فعال‌سازی محیط مجازی (اگر از venv استفاده می‌کنید)
source venv/bin/activate  # macOS/Linux
# یا
venv\Scripts\activate  # Windows

# نصب پکیج‌ها
pip install -r requirements.txt
```

### 4. اجرای سرور

```bash
python main.py
```

یا با uvicorn:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### 5. دسترسی به رابط کاربری

مرورگر را باز کنید و به آدرس زیر بروید:

```
http://localhost:8000
```

## پیکربندی

### تنظیم آدرس Ollama

به صورت پیش‌فرض، برنامه به `http://localhost:11434` متصل می‌شود. اگر Ollama شما روی آدرس دیگری اجرا می‌شود، می‌توانید با متغیر محیطی تنظیم کنید:

```bash
export OLLAMA_HOST=http://localhost:11434
python main.py
```

### افزودن مدل‌های جدید

فایل `models.json` را ویرایش کنید و مدل‌های مورد نظر خود را اضافه کنید:

```json
{
  "models": [
    {
      "id": "qwen3",
      "name": "Qwen 3",
      "description": "مدل Qwen 3 برای چت و تولید متن"
    },
    {
      "id": "llama3.2",
      "name": "Llama 3.2",
      "description": "مدل Llama 3.2"
    }
  ],
  "default": "qwen3"
}
```

## استفاده

1. **انتخاب مدل**: از منوی سمت راست یک مدل انتخاب کنید
2. **ارسال پیام**: پیام خود را در باکس ورودی تایپ کنید و Enter بزنید یا روی دکمه ارسال کلیک کنید
3. **پرامپت سیستم**: برای تنظیم پرامپت سیستم، روی "پرامپت سیستم" کلیک کنید
4. **توقف پاسخ**: در حین دریافت پاسخ، می‌توانید با کلیک روی دکمه توقف، پاسخ را متوقف کنید
5. **چت جدید**: برای شروع یک چت جدید، روی دکمه "چت جدید" کلیک کنید

## ساختار پروژه

```
Ollama-UI/
├── main.py              # سرور FastAPI
├── models.json          # فایل مدل‌های موجود
├── requirements.txt     # وابستگی‌های Python
├── static/
│   ├── index.html      # صفحه اصلی
│   ├── styles.css      # استایل‌ها
│   └── app.js          # منطق فرانت‌اند
└── README.md           # این فایل
```

## عیب‌یابی

### مشکل: "خطا در ارتباط با Ollama"

- مطمئن شوید Ollama در حال اجرا است: `ollama serve`
- بررسی کنید که مدل مورد نظر دانلود شده باشد: `ollama list`
- اگر Ollama روی پورت دیگری اجرا می‌شود، متغیر محیطی `OLLAMA_HOST` را تنظیم کنید

### مشکل: مدل‌ها نمایش داده نمی‌شوند

- فایل `models.json` را بررسی کنید
- مطمئن شوید مدل‌های تعریف شده در Ollama نصب شده‌اند

### مشکل: پاسخ‌ها استریم نمی‌شوند

- بررسی کنید که WebSocket به درستی متصل شده باشد (Console مرورگر را بررسی کنید)
- مطمئن شوید که Ollama از استریمینگ پشتیبانی می‌کند

## مجوز

این پروژه برای استفاده شخصی و آموزشی است.

## مشارکت

برای گزارش باگ یا پیشنهاد ویژگی جدید، لطفاً Issue ایجاد کنید.
