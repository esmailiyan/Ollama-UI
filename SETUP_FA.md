# راهنمای نصب و پیکربندی Ollama UI

## مرحله 1: نصب Ollama

### macOS
```bash
brew install ollama
```

### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows
از [ollama.com](https://ollama.com) دانلود و نصب کنید.

## مرحله 2: راه‌اندازی Ollama

### شروع سرویس Ollama

در یک ترمینال، دستور زیر را اجرا کنید:

```bash
ollama serve
```

این دستور سرویس Ollama را روی پورت پیش‌فرض `11434` راه‌اندازی می‌کند.

### دانلود مدل Qwen3

در یک ترمینال جدید (یا همان ترمینال اگر سرویس را در background اجرا کردید):

```bash
ollama pull qwen3
```

این دستور مدل Qwen3 را دانلود می‌کند. بسته به سرعت اینترنت شما، این فرآیند ممکن است چند دقیقه طول بکشد.

### بررسی مدل‌های نصب شده

برای دیدن لیست مدل‌های نصب شده:

```bash
ollama list
```

## مرحله 3: راه‌اندازی Ollama UI

### روش 1: استفاده از اسکریپت (پیشنهادی)

```bash
./run.sh
```

### روش 2: دستی

```bash
# فعال‌سازی محیط مجازی
source venv/bin/activate

# نصب وابستگی‌ها (اگر قبلاً نصب نشده)
pip install -r requirements.txt

# راه‌اندازی سرور
python main.py
```

یا با uvicorn:

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

## مرحله 4: دسترسی به رابط کاربری

مرورگر را باز کنید و به آدرس زیر بروید:

```
http://localhost:8000
```

## پیکربندی پیشرفته

### تغییر آدرس Ollama

اگر Ollama شما روی آدرس یا پورت دیگری اجرا می‌شود:

```bash
export OLLAMA_HOST=http://localhost:11434
python main.py
```

یا در Windows:

```cmd
set OLLAMA_HOST=http://localhost:11434
python main.py
```

### افزودن مدل‌های بیشتر

1. مدل را در Ollama دانلود کنید:
   ```bash
   ollama pull llama3.2
   ollama pull mistral
   ```

2. فایل `models.json` را ویرایش کنید و مدل جدید را اضافه کنید:

```json
{
  "models": [
    {
      "id": "qwen3",
      "name": "Qwen 3",
      "description": "مدل Qwen 3 برای چت و تولید متن"
    },
    {
      "id": "llama3.2",
      "name": "Llama 3.2",
      "description": "مدل Llama 3.2"
    }
  ],
  "default": "qwen3"
}
```

## عیب‌یابی

### مشکل: "خطا در ارتباط با Ollama"

**راه‌حل:**
1. مطمئن شوید Ollama در حال اجرا است:
   ```bash
   # بررسی سرویس
   curl http://localhost:11434/api/tags
   ```
   
   اگر خطا داد، Ollama را راه‌اندازی کنید:
   ```bash
   ollama serve
   ```

2. بررسی کنید که مدل نصب شده باشد:
   ```bash
   ollama list
   ```
   
   اگر مدل وجود ندارد:
   ```bash
   ollama pull qwen3
   ```

### مشکل: مدل‌ها در UI نمایش داده نمی‌شوند

**راه‌حل:**
1. فایل `models.json` را بررسی کنید
2. مطمئن شوید مدل‌های تعریف شده در Ollama نصب شده‌اند
3. Console مرورگر را بررسی کنید (F12) برای خطاهای JavaScript

### مشکل: پاسخ‌ها استریم نمی‌شوند

**راه‌حل:**
1. Console مرورگر را باز کنید (F12) و بررسی کنید که WebSocket متصل شده باشد
2. مطمئن شوید که Ollama از استریمینگ پشتیبانی می‌کند (نسخه جدید)
3. بررسی کنید که فایروال یا proxy مانع ارتباط WebSocket نشده باشد

### مشکل: پرامپت سیستم کار نمی‌کند

**راه‌حل:**
1. مطمئن شوید که پرامپت را در باکس مربوطه وارد کرده‌اید
2. بعد از وارد کردن پرامپت، باکس را ببندید
3. پرامپت برای تمام پیام‌های بعدی اعمال می‌شود

## نکات مهم

1. **سرویس Ollama باید همیشه در حال اجرا باشد** - اگر Ollama را ببندید، UI نمی‌تواند با مدل‌ها ارتباط برقرار کند

2. **مدل‌ها فضای زیادی اشغال می‌کنند** - مدل Qwen3 حدود 2-4 GB فضا نیاز دارد

3. **اولین استفاده ممکن است کند باشد** - مدل‌ها برای اولین بار باید در حافظه بارگذاری شوند

4. **استفاده از GPU** - اگر GPU دارید، Ollama به صورت خودکار از آن استفاده می‌کند

## تست اتصال

برای تست اینکه همه چیز به درستی کار می‌کند:

```bash
# تست API Ollama
curl http://localhost:11434/api/tags

# تست مدل
ollama run qwen3 "سلام"
```

اگر این دستورات کار کردند، همه چیز آماده است!

## پشتیبانی

اگر مشکلی پیش آمد:
1. Console مرورگر را بررسی کنید (F12)
2. لاگ‌های سرور را بررسی کنید
3. مطمئن شوید Ollama در حال اجرا است
